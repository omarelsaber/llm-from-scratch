# ğŸ§  LLM from Scratch

This repository documents my journey of building a **Large Language Model (LLM) from scratch**, inspired by *Sebastian Raschka's book: "Build a Large Language Model (From Scratch)"*.  

The project includes both **theory (documentation)** and **hands-on implementation (code + experiments)**.  
Each chapter is implemented step by step with Jupyter Notebooks, reusable Python modules, and unit tests.

---

## ğŸ“‚ Project Structure
llm-from-scratch/
â”‚
â”œâ”€â”€ data/ # Text datasets (e.g., The Verdict by Edith Wharton)
â”œâ”€â”€ notebooks/ # Jupyter notebooks for each chapter
â”‚ â””â”€â”€ 02-tokenization-and-embeddings.ipynb
â”œâ”€â”€ src/ # Core Python code (tokenizer, dataset, embeddings, etc.)
â”‚ â”œâ”€â”€ tokenizer.py
â”‚ â”œâ”€â”€ dataset.py
â”‚ â””â”€â”€ embeddings.py
â”œâ”€â”€ tests/ # Unit tests for reproducibility
â”‚ â””â”€â”€ test_tokenizer.py
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ LICENSE

---

## ğŸš€ Progress
- [x] **Chapter 2**: Tokenization & Embeddings
  - Load and preprocess raw text
  - Implement regex-based tokenization
  - Build BPE tokenizer (using `tiktoken`)
  - Create a sliding-window dataset with PyTorch
  - Implement token embeddings + positional embeddings

âœ… Notebook: [`02-tokenization-and-embeddings.ipynb`](notebooks/02-tokenization-and-embeddings.ipynb)

---

## ğŸ› ï¸ Tech Stack
- Python 3.11+
- PyTorch
- tiktoken (OpenAIâ€™s BPE tokenizer)
- Jupyter Notebooks
- pytest (for testing)

---

## ğŸ“Œ Next Steps
- [ ] Chapter 3: Self-Attention and the Transformer architecture
- [ ] Build a minimal GPT model
- [ ] Training loop & optimization
- [ ] Scaling up to larger models

---

## ğŸ“– References
- *Build a Large Language Model (From Scratch)* by Sebastian Raschka (Manning, 2024)  
- OpenAIâ€™s [tiktoken library](https://github.com/openai/tiktoken)  
- PyTorch documentation  

---

## ğŸ¤ Contribution
This repo is mainly for **learning and experimentation**.  
But feel free to open issues or PRs if you have suggestions for improvements!
